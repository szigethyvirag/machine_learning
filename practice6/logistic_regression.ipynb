{"cells":[{"cell_type":"code","execution_count":32,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-02-25T20:42:49.193772Z","iopub.status.busy":"2022-02-25T20:42:49.193132Z","iopub.status.idle":"2022-02-25T20:42:49.205392Z","shell.execute_reply":"2022-02-25T20:42:49.204382Z","shell.execute_reply.started":"2022-02-25T20:42:49.193719Z"},"trusted":true},"outputs":[],"source":["\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","data=pd.read_csv('C:/Users/Virag/Desktop/Machine Learning/practice6/data/heart_disease.csv')\n","labels=data.values[:,-1]\n","labels[labels>1]=1\n","labels=labels.astype(int)\n","\n","data=data.values[:,:-1]"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-02-25T20:42:49.207562Z","iopub.status.busy":"2022-02-25T20:42:49.206998Z","iopub.status.idle":"2022-02-25T20:42:49.212775Z","shell.execute_reply":"2022-02-25T20:42:49.211944Z","shell.execute_reply.started":"2022-02-25T20:42:49.207528Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(303, 105)\n"]}],"source":["data=PolynomialFeatures(2).fit_transform(data)\n","print(data.shape)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-02-25T20:42:49.214166Z","iopub.status.busy":"2022-02-25T20:42:49.213826Z","iopub.status.idle":"2022-02-25T20:42:49.224289Z","shell.execute_reply":"2022-02-25T20:42:49.223597Z","shell.execute_reply.started":"2022-02-25T20:42:49.214138Z"},"trusted":true},"outputs":[],"source":["#Standardize data (substract mean divide with std)\n","data=(data-np.mean(data))/np.std(data)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-02-25T20:42:49.225704Z","iopub.status.busy":"2022-02-25T20:42:49.225392Z","iopub.status.idle":"2022-02-25T20:42:49.235625Z","shell.execute_reply":"2022-02-25T20:42:49.234751Z","shell.execute_reply.started":"2022-02-25T20:42:49.225674Z"},"trusted":true},"outputs":[],"source":["def train_test_split(data,labels,test_ratio=0.2):\n","    idxs=np.arange(data.shape[0])\n","    np.random.shuffle(idxs)\n","    test_idxs=idxs[:round(len(idxs)*test_ratio)]\n","    train_idxs=idxs[round(len(idxs)*test_ratio):]\n","    return data[train_idxs],labels[train_idxs],data[test_idxs],labels[test_idxs]\n","    "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-02-25T20:42:49.237570Z","iopub.status.busy":"2022-02-25T20:42:49.237181Z","iopub.status.idle":"2022-02-25T20:42:49.247595Z","shell.execute_reply":"2022-02-25T20:42:49.246863Z","shell.execute_reply.started":"2022-02-25T20:42:49.237538Z"},"trusted":true},"outputs":[],"source":["def visualize(data, labels,predictor):\n","    import matplotlib.pyplot as plt\n","    min1, max1 = data[:, 0].min()-data[:, 0].min()*0.1, data[:, 0].max()+data[:, 0].max()*0.1\n","    min2, max2 = data[:, 1].min()-data[:, 1].min()*0.1, data[:, 1].max()+data[:, 1].max()*0.1\n","    # define the x and y scale\n","    x1grid = np.arange(min1, max1, np.abs(max1-min1)*0.001)\n","    x2grid = np.arange(min2, max2, np.abs(max2-min2)*0.001)\n","    # create all of the lines and rows of the grid\n","    xx, yy = np.meshgrid(x1grid, x2grid)\n","    # flatten each grid to a vector\n","    r1, r2 = xx.flatten(), yy.flatten()\n","    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n","    # horizontal stack vectors to create x1,x2 input for the model\n","    grid = np.hstack((r1,r2))\n","    # make predictions for the grid\n","    yhat = predictor.predict(grid)\n","    # reshape the predictions back into a grid\n","    zz = yhat.reshape(xx.shape)\n","    # plot the grid of x, y and z values as a surface\n","    plt.contourf(xx, yy, zz, cmap='Paired')\n","    # create scatter plot for samples from each class\n","    for class_value in np.unique(labels):\n","        # get row indexes for samples with this class\n","        row_ix = np.where(labels == class_value)\n","        # create scatter of these samples\n","        plt.scatter(data[row_ix, 0], data[row_ix, 1], cmap='Paired')\n","    plt.tight_layout()\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-02-25T20:42:49.249109Z","iopub.status.busy":"2022-02-25T20:42:49.248761Z","iopub.status.idle":"2022-02-25T20:42:49.266168Z","shell.execute_reply":"2022-02-25T20:42:49.265420Z","shell.execute_reply.started":"2022-02-25T20:42:49.249082Z"},"trusted":true},"outputs":[],"source":["class LogisticRegression():\n","    def __init__(self):\n","        self.w = None\n","    def fit(self,data,labels,test_data,test_labels,max_iterations=500,lamb = 0.1):\n","        X=data\n","        Y=labels\n","        step_size=0.05\n","        batch_size=32\n","        N = X.shape[0]\n","        Y = Y.squeeze()\n","        assert Y.shape == (N,), (Y.shape, N)\n","\n","        def gen_batches():\n","            inds = np.arange(N)\n","            np.random.shuffle(inds)\n","            if batch_size is None:\n","                yield inds\n","            else:\n","                for i in range(0, N, batch_size):\n","                    yield inds[i:i + batch_size]\n","\n","        # Initialise w\n","        w = np.random.randn(X.shape[1])\n","        for it in range(max_iterations):\n","            # Train on the permuted dataset\n","            avg_error = 0\n","            for batch_inds in gen_batches():\n","                x = X[batch_inds, ...]\n","                y = Y[batch_inds]\n","                y_hat = self.predict_proba(x, w)\n","                e_in_hat = self.binary_cross_entropy(y,y_hat)\n","                gradient_hat = self.error_gradient(x, y,y_hat)\n","                avg_error += e_in_hat\n","                # Update\n","                w -= step_size * (gradient_hat + lamb) #2w in L2 and 1 in L1\n","            avg_error /= N\n","            Y_hat = (self.predict_proba(X, w) > 0.5).astype(int)\n","            accuracy = self.accuracy(Y,Y_hat)\n","            test_Y_hat= (self.predict_proba(test_data, w) > 0.5).astype(int)\n","            test_accuracy = self.accuracy(test_labels,test_Y_hat)\n","            #print(f'Iteration #{it}: average error: {avg_error:0.2f}  '\n","            #      f'train accuracy: {accuracy:0.02f}'\n","            #     f'test accuracy: {test_accuracy:0.02f}')\n","        self.w=w\n","        #print(self.w)\n","\n","\n","    def sigmoid(self,data):\n","        # return 1 / (1 + np.exp(-h))\n","        # Numerically stable version, see:\n","        # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n","        h=data\n","        mask = h >= 0\n","        result = np.zeros_like(h)\n","        z = np.exp(-h[mask])\n","        result[mask] = 1 / (1+z)\n","        z = np.exp(h[~mask])\n","        result[~mask] = z / (1+z)\n","        return result\n","\n","    def binary_cross_entropy(self,true,prediction):\n","        return np.sum(-np.log(true*prediction + (1-true) * (1-prediction) + 1e-6))\n","\n","    def error_gradient(self,data,true,prediction):\n","        N = data.shape[0]\n","        assert prediction.shape == (N,), prediction.shape\n","        assert true.shape == (N,), true.shape\n","        assert np.all((true == 0) | (true == 1))\n","        t = prediction - true\n","        t.shape = (N, 1)\n","        elementwise_gradient = t * data\n","        return np.mean(elementwise_gradient, axis=0)\n","\n","    def predict_proba(self,data,w=[]):\n","        if len(w)>0:pass\n","        else: w=self.w\n","        return self.sigmoid(data @ w)\n","    def predict(self,data):\n","        return np.rint(self.sigmoid(data @ self.w))\n","    def accuracy(self,true,prediction):\n","        return np.mean(true == prediction)\n"," "]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6735537190082644\n","0.6229508196721312\n","0.7107438016528925\n","0.7377049180327869\n","0.6074380165289256\n","0.6229508196721312\n"]}],"source":["#with L1 regularization:\n","train_data,train_labels,test_data,test_labels = train_test_split(data,labels)\n","for i in [0.000001, 0.0001, 0.1]:\n","    lr=LogisticRegression()\n","    #print(\"lambda: \" + str(i))\n","    lr.fit(train_data,train_labels,test_data,test_labels,lamb=i)\n","    prediction_in=lr.predict(train_data)\n","    prediction_out=lr.predict(test_data)\n","    print(lr.accuracy(train_labels,prediction_in))\n","    print(lr.accuracy(test_labels,prediction_out))"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-02-25T20:42:49.267489Z","iopub.status.busy":"2022-02-25T20:42:49.267271Z","iopub.status.idle":"2022-02-25T20:42:49.840600Z","shell.execute_reply":"2022-02-25T20:42:49.839759Z","shell.execute_reply.started":"2022-02-25T20:42:49.267464Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.743801652892562\n","0.6885245901639344\n","0.7355371900826446\n","0.6721311475409836\n","0.6487603305785123\n","0.6065573770491803\n"]}],"source":["#With L2 regularization:\n","train_data,train_labels,test_data,test_labels = train_test_split(data,labels)\n","for i in [0.000001, 0.0001, 0.1]:\n","    lr=LogisticRegression()\n","    #print(\"lambda: \" + str(i))\n","    lr.fit(train_data,train_labels,test_data,test_labels,lamb=i)\n","    prediction_in=lr.predict(train_data)\n","    prediction_out=lr.predict(test_data)\n","    print(lr.accuracy(train_labels,prediction_in))\n","    print(lr.accuracy(test_labels,prediction_out))"]},{"cell_type":"markdown","metadata":{},"source":["The accuracy with L1 regularization was better than with L2 regularization."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
